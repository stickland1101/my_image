{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlyptNJ2ICnN"
      },
      "source": [
        "# Fooooooooooooooocus\n",
        "Colab NoteBook Created by [licyk](https://github.com/licyk)\n",
        "\n",
        "Jupyter Notebook 仓库：[licyk/sd-webui-all-in-one](https://github.com/licyk/sd-webui-all-in-one)\n",
        "\n",
        "这是适用于 [Colab](https://colab.research.google.com) 部署 Fooocus 的 Jupyter Notebook，使用时请按顺序执行 Jupyter Notebook 单元。\n",
        "\n",
        "Colab 链接：<a href=\"https://colab.research.google.com/github/licyk/sd-webui-all-in-one/blob/main/fooocus_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## 功能\n",
        "1. 环境配置：配置安装的 PyTorch 版本、内网穿透的方式，并安装 [Fooocus](https://github.com/lllyasviel/Fooocus)，默认设置下已启用`配置环境完成后立即启动 Fooocus`选项，则安装完成后将直接启动 Fooocus。\n",
        "2. 下载模型：下载可选列表中的模型（可选）。\n",
        "3. 自定义模型下载：使用链接下载模型（可选）。\n",
        "4. 启动 Fooocus：启动 Fooocus，并显示访问地址。\n",
        "\n",
        "\n",
        "## 使用\n",
        "1. 在 Colab -> 代码执行程序 > 更改运行时类型 -> 硬件加速器 选择`GPU T4`或者其他 GPU。\n",
        "2. `环境配置`单元中的选项通常不需要修改，保持默认即可。\n",
        "3. 运行`环境配置`单元，默认设置下已启用`配置环境完成后立即启动 Fooocus`选项，则环境配置完成后立即启动 Fooocus，此时将弹出 Google Drive 授权页面，根据提示进行操作。配置完成后将启动 Fooocus，Fooocus 的访问地址可在日志中查看。\n",
        "4. 如果未启用`配置环境完成后立即启动 Fooocus`选项，需要运行`启动 Fooocus`单元，此时将弹出 Google Drive 授权页面，根据提示进行操作。配置完成后将启动 Fooocus，Fooocus 的访问地址可在日志中查看。\n",
        "\n",
        "## 提示\n",
        "1. Colab 在关机后将会清除所有数据，所以每次重新启动时必须运行`环境配置`单元才能运行`启动 Fooocus`单元。\n",
        "2. [Ngrok](https://ngrok.com) 内网穿透在使用前需要填写 Ngrok Token，可在 [Ngrok](https://ngrok.com) 官网获取。\n",
        "3. 每次启动 Colab 后必须运行`环境配置`单元，才能运行`启动 Fooocus`单元启动 Fooocus。\n",
        "4. 其他功能有自定义模型下载等功能，根据自己的需求进行使用。\n",
        "5. 运行`启动 Fooocus`时将弹出 Google Drive 访问授权提示，根据提示进行授权。授权后，使用 Fooocus 生成的图片将保存在 Google Drive 的`fooocus_output`文件夹中。\n",
        "6. Gradio 的内网穿透地址可在启动 Fooocus 后日志中的`Running on public URL`查看。\n",
        "7. Fooocus 启动预设文件可自行编写，并上传到 Github 或者其他平台并获取下载链接后，可以替换`Fooocus 启动预设文件地址`中的链接，进行环境配置时将使用该预设文件启动 Fooocus。预设文件可自行下载下来进行参考，其他有关说明可阅读：[Customization - lllyasviel/Fooocus](https://github.com/lllyasviel/Fooocus?tab=readme-ov-file#customization)。\n",
        "8. 在 Colab 的文件浏览器中，可临时上传 LoRA 模型，在`Fooocus`->`models`中找到`loras`文件夹，右键可以打开上传文件菜单，上传 LoRA 模型完成后在 Fooocus 中刷新模型列表即可使用，其他模型同理。注意，Colab 在关机后将会清除数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "VjYy0F2gZIPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3651e48-5343-4500-b2fd-abd999061d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: Fooocus 运行环境配置完成\n"
          ]
        }
      ],
      "source": [
        "#@title 👇 环境配置\n",
        "from typing import Union, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "def echo(*args):\n",
        "    \"\"\"格式化消息输出\"\"\"\n",
        "    for i in args:\n",
        "        print(f\":: {i}\")\n",
        "\n",
        "\n",
        "\n",
        "class ARIA2:\n",
        "    \"\"\"基于 Aria2 的文件下载工具\"\"\"\n",
        "\n",
        "    def __init__(self, workspace: Union[str, Path], workfolder: str) -> None:\n",
        "        \"\"\"基于 Aria2 的文件下载工具\n",
        "\n",
        "        参数:\n",
        "            workspace (`str`, `Path`):\n",
        "                工作区路径\n",
        "\n",
        "            workfolder (`str`):\n",
        "                工作区的文件夹名称\n",
        "        \"\"\"\n",
        "        self.WORKSPACE = workspace\n",
        "        self.WORKFOLDER = workfolder\n",
        "\n",
        "\n",
        "    def aria2(self, url: str, path: Union[str, Path], filename: str) -> str:\n",
        "        \"\"\"调用 Aria2 下载文件\n",
        "\n",
        "        参数:\n",
        "            url (`str`):\n",
        "                文件的下载链接\n",
        "\n",
        "            path (`str`, `Path`):\n",
        "                将文件下载到本地的路径\n",
        "\n",
        "            filename (`str`):\n",
        "                将要下载的文件重命名\n",
        "\n",
        "        返回值:\n",
        "            `str`: 文件保存路径\n",
        "        \"\"\"\n",
        "        import os\n",
        "        file_path = os.path.join(path, filename)\n",
        "        if not os.path.exists(file_path):\n",
        "            echo(f\"开始下载 {filename} ，路径: {file_path}\")\n",
        "            !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}\" -d \"{path}\" -o \"{filename}\"\n",
        "            if os.path.exists(file_path) and not os.path.exists(file_path + \".aria2\"):\n",
        "                echo(f\"{filename} 下载完成\")\n",
        "                return file_path\n",
        "            else:\n",
        "                echo(f\"{filename} 下载中断\")\n",
        "                return None\n",
        "        else:\n",
        "            if os.path.exists(file_path + \".aria2\"):\n",
        "                echo(f\"开始下载 {filename} ，路径: {path}/{filename}\")\n",
        "                !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}\" -d \"{path}\" -o \"{filename}\"\n",
        "                if os.path.exists(file_path) and not os.path.exists(file_path + \".aria2\"):\n",
        "                    echo(f\"{filename} 下载完成\")\n",
        "                    return file_path\n",
        "                else:\n",
        "                    echo(f\"{filename} 下载中断\")\n",
        "                    return None\n",
        "            else:\n",
        "                echo(f\"{filename} 文件已存在，路径: {file_path}\")\n",
        "                return file_path\n",
        "\n",
        "\n",
        "\n",
        "# GIT\n",
        "class GIT:\n",
        "    \"\"\"基于 Git 命令的模块\"\"\"\n",
        "\n",
        "    def __init__(self, workspace: Union[str, Path], workfolder: str) -> None:\n",
        "        \"\"\"基于 Git 命令的模块\n",
        "\n",
        "        参数:\n",
        "            workspace (`str`, `Path`):\n",
        "                工作区路径\n",
        "\n",
        "            workfolder (`str`):\n",
        "                工作区的文件夹名称\n",
        "        \"\"\"\n",
        "        self.WORKSPACE = workspace\n",
        "        self.WORKFOLDER = workfolder\n",
        "\n",
        "\n",
        "    def exists(self, addr: Optional[str]= None, path: Optional[Union[str, Path]]= None, name: Optional[str]= None) -> bool:\n",
        "        \"\"\"检测要克隆的项目是否存在于指定路径\n",
        "\n",
        "        参数:\n",
        "            addr (`str`, `None`):\n",
        "                Git 仓库的地址\n",
        "\n",
        "            path (`str`, `Path`, `None`):\n",
        "                将 Git 仓库下载到本地的路径\n",
        "\n",
        "            name (`str`, `None`):\n",
        "                将 Git 仓库进行重命名\n",
        "\n",
        "        返回值:\n",
        "            `bool`: Git 仓库存在时则返回`True`, 否则返回`False`\n",
        "        \"\"\"\n",
        "        import os\n",
        "        if addr is not None:\n",
        "            if path is None and name is None:\n",
        "                path = os.path.join(os.getcwd(), addr.split(\"/\").pop().split(\".git\", 1)[0])\n",
        "            elif path is None and name is not None:\n",
        "                path = os.path.join(os.getcwd(), name)\n",
        "            elif path is not None and name is None:\n",
        "                path = os.path.join(os.path.normpath(path), addr.split(\"/\").pop().split(\".git\", 1)[0])\n",
        "            elif path is not None and name is not None:\n",
        "                path = os.path.join(os.path.normpath(path), name)\n",
        "\n",
        "        if os.path.exists(path):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def clone(self, addr: str, path: Optional[Union[str, Path]]= None, name: Optional[str]= None) -> str:\n",
        "        \"\"\"克隆 Git 仓库到本地\n",
        "\n",
        "        参数:\n",
        "            addr (`str`):\n",
        "                Git 仓库的地址\n",
        "\n",
        "            path (`str`, `Path`, `None`):\n",
        "                将 Git 仓库下载到本地的路径\n",
        "\n",
        "            name (`str`, `None`):\n",
        "                将 Git 仓库进行重命名\n",
        "\n",
        "        返回值:\n",
        "            `str`: 下载的仓库地址\n",
        "        \"\"\"\n",
        "        import os\n",
        "        repo = addr.split(\"/\").pop().split(\".git\", 1)[0]\n",
        "        if path is None and name is None:\n",
        "            path = os.getcwd()\n",
        "            name = repo\n",
        "        elif path is not None and name is None:\n",
        "            name = repo\n",
        "        elif path is None and name is not None:\n",
        "            path = os.getcwd()\n",
        "\n",
        "        repo_path = os.path.join(path, name)\n",
        "\n",
        "        if not self.exists(addr, path, name):\n",
        "            echo(f\"开始下载 {repo}\")\n",
        "            !git clone {addr} \"{repo_path}\" --recurse-submodules\n",
        "            if os.path.exists(repo_path):\n",
        "                echo(f\"{repo} 下载成功, 路径: {repo_path}\")\n",
        "                return repo_path\n",
        "            else:\n",
        "                echo(f\"{repo} 下载失败\")\n",
        "                return None\n",
        "        else:\n",
        "            echo(f\"{repo} 已存在, 路径: {repo_path}\")\n",
        "            return repo_path\n",
        "\n",
        "\n",
        "\n",
        "class TUNNEL:\n",
        "    \"\"\"内网穿透工具\"\"\"\n",
        "    LOCALHOST_RUN = \"localhost.run\"\n",
        "    REMOTE_MOE = \"remote.moe\"\n",
        "    PINGGY_IO = \"a.pinggy.io\"\n",
        "\n",
        "    def __init__(self, workspace: Union[str, Path], workfolder: str, port: int) -> None:\n",
        "        \"\"\"提供初始化环境的功能\n",
        "\n",
        "        参数:\n",
        "            workspace (`str`, `Path`):\n",
        "                工作区路径\n",
        "\n",
        "            workfolder (`str`):\n",
        "                工作区的文件夹名称\n",
        "\n",
        "            port (`int`):\n",
        "                要进行端口映射的端口\n",
        "        \"\"\"\n",
        "        self.WORKSPACE = workspace\n",
        "        self.WORKFOLDER = workfolder\n",
        "        self.PORT = port\n",
        "\n",
        "\n",
        "    def ngrok(self, ngrok_token: str) -> Union[str, None]:\n",
        "        \"\"\"使用 Ngrok 内网穿透\n",
        "\n",
        "        参数:\n",
        "            ngrok_token (`str`):\n",
        "                Ngrok 账号 Token\n",
        "\n",
        "        返回值:\n",
        "            `str`:\n",
        "                Ngrok 内网穿透生成的访问地址\n",
        "        \"\"\"\n",
        "        from pyngrok import conf, ngrok\n",
        "        conf.get_default().auth_token = ngrok_token\n",
        "        conf.get_default().monitor_thread = False\n",
        "        port = self.PORT\n",
        "        try:\n",
        "            ssh_tunnels = ngrok.get_tunnels(conf.get_default())\n",
        "        except Exception as e:\n",
        "            echo(\"启动 Ngrok 内网穿透时出现了错误\", e)\n",
        "            return None\n",
        "        if len(ssh_tunnels) == 0:\n",
        "            ssh_tunnel = ngrok.connect(port, bind_tls=True)\n",
        "            return ssh_tunnel.public_url\n",
        "        else:\n",
        "            return ssh_tunnels[0].public_url\n",
        "\n",
        "\n",
        "    def cloudflare(self) -> Union[str, None]:\n",
        "        \"\"\"使用 CloudFlare 内网穿透\n",
        "\n",
        "        返回值:\n",
        "            `str`:\n",
        "                CloudFlare 内网穿透生成的访问地址\n",
        "        \"\"\"\n",
        "        from pycloudflared import try_cloudflare\n",
        "        port = self.PORT\n",
        "        try:\n",
        "            return try_cloudflare(port).tunnel\n",
        "        except Exception as e:\n",
        "            echo(\"启动 CloudFlare 内网穿透时出现了错误\", e)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def gen_key(self, path: Union[str, Path]) -> None:\n",
        "        \"\"\"生成 SSH 密钥\n",
        "\n",
        "        参数:\n",
        "            path (`str`, `Path`):\n",
        "                生成 SSH 密钥的路径\n",
        "        \"\"\"\n",
        "        import subprocess\n",
        "        import shlex\n",
        "        from pathlib import Path\n",
        "        path = Path(path)\n",
        "        arg_string = f'ssh-keygen -t rsa -b 4096 -N \"\" -q -f {path.as_posix()}'\n",
        "        args = shlex.split(arg_string)\n",
        "        subprocess.run(args, check=True)\n",
        "        path.chmod(0o600)\n",
        "\n",
        "\n",
        "    def ssh_tunnel(self, host: str) -> Union[str, None]:\n",
        "        \"\"\"使用 SSH 进行内网穿透\n",
        "\n",
        "        参数:\n",
        "            host (`str`):\n",
        "                提供 SSH 内网穿透的服务器地址\n",
        "\n",
        "        返回值:\n",
        "            (`str`, `None`):\n",
        "                使用内网穿透得到的访问地址, 如果启动内网穿透失败则不返回地址\n",
        "        \"\"\"\n",
        "        import subprocess\n",
        "        import atexit\n",
        "        import shlex\n",
        "        import re\n",
        "        import os\n",
        "        from pathlib import Path\n",
        "        from tempfile import TemporaryDirectory\n",
        "\n",
        "        ssh_name = \"id_rsa\"\n",
        "        ssh_path = Path(self.WORKSPACE) / ssh_name\n",
        "        port = self.PORT\n",
        "\n",
        "        tmp = None\n",
        "        if not ssh_path.exists():\n",
        "            try:\n",
        "                self.gen_key(ssh_path)\n",
        "            # write permission error or etc\n",
        "            except subprocess.CalledProcessError:\n",
        "                tmp = TemporaryDirectory()\n",
        "                ssh_path = Path(tmp.name) / ssh_name\n",
        "                self.gen_key(ssh_path)\n",
        "\n",
        "        arg_string = f\"ssh -R 80:127.0.0.1:{port} -o StrictHostKeyChecking=no -i {ssh_path.as_posix()} {host}\"\n",
        "        args = shlex.split(arg_string)\n",
        "\n",
        "        tunnel = subprocess.Popen(\n",
        "            args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, encoding=\"utf-8\"\n",
        "        )\n",
        "\n",
        "        atexit.register(tunnel.terminate)\n",
        "        if tmp is not None:\n",
        "            atexit.register(tmp.cleanup)\n",
        "\n",
        "        tunnel_url = \"\"\n",
        "        localhostrun_pattern = re.compile(r\"(?P<url>https?://\\S+\\.lhr\\.life)\")\n",
        "        remotemoe_pattern = re.compile(r\"(?P<url>https?://\\S+\\.remote\\.moe)\")\n",
        "        pinggyio_pattern = re.compile(r\"(?P<url>https?://\\S+\\.pinggy\\.link)\")\n",
        "        if host == self.LOCALHOST_RUN:\n",
        "            pattern = localhostrun_pattern\n",
        "            lines = 27\n",
        "        elif host == self.REMOTE_MOE:\n",
        "            pattern = remotemoe_pattern\n",
        "            lines = 10\n",
        "        elif host == self.PINGGY_IO:\n",
        "            pattern = pinggyio_pattern\n",
        "            lines = 10\n",
        "\n",
        "        for _ in range(lines):\n",
        "            line = tunnel.stdout.readline()\n",
        "            if line.startswith(\"Warning\"):\n",
        "                print(line, end=\"\")\n",
        "\n",
        "            url_match = pattern.search(line)\n",
        "            if url_match:\n",
        "                return url_match.group(\"url\")\n",
        "        else:\n",
        "            echo(f\"启动 {host} 内网穿透失败\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def localhost_run(self) -> Union[str, None]:\n",
        "        \"\"\"使用 localhost.run 进行内网穿透\n",
        "\n",
        "        返回值:\n",
        "            (`str`, `None`):\n",
        "                使用内网穿透得到的访问地址, 如果启动内网穿透失败则不返回地址\n",
        "        \"\"\"\n",
        "        urls = self.ssh_tunnel(self.LOCALHOST_RUN)\n",
        "        return urls\n",
        "\n",
        "\n",
        "    def remote_moe(self) -> Union[str, None]:\n",
        "        \"\"\"使用 remote.moe 进行内网穿透\n",
        "\n",
        "        返回值:\n",
        "            (`str`, `None`):\n",
        "                使用内网穿透得到的访问地址, 如果启动内网穿透失败则不返回地址\n",
        "        \"\"\"\n",
        "        urls = self.ssh_tunnel(self.REMOTE_MOE)\n",
        "        return urls\n",
        "\n",
        "    def pinggy_io(self) -> Union[str, None]:\n",
        "        \"\"\"使用 pinggy.io 进行内网穿透\n",
        "\n",
        "        返回值:\n",
        "            (`str`, `None`):\n",
        "                使用内网穿透得到的访问地址, 如果启动内网穿透失败则不返回地址\n",
        "        \"\"\"\n",
        "        urls = self.ssh_tunnel(self.PINGGY_IO)\n",
        "        return urls\n",
        "\n",
        "\n",
        "    def gradio(self) -> Union[str, None]:\n",
        "        \"\"\"使用 Gradio 进行内网穿透\n",
        "\n",
        "        返回值:\n",
        "            (`str`, `None`):\n",
        "                使用内网穿透得到的访问地址, 如果启动内网穿透失败则不返回地址\n",
        "        \"\"\"\n",
        "        import secrets\n",
        "        try:\n",
        "            from gradio_tunneling.main import setup_tunnel\n",
        "        except Exception as e:\n",
        "            echo(f\"导入 Gradio Tunneling 模块时出现错误\", e)\n",
        "            return None\n",
        "\n",
        "        subdomain = secrets.token_urlsafe(32)\n",
        "        port = self.PORT\n",
        "\n",
        "        try:\n",
        "            tunnel_url = setup_tunnel(\n",
        "                local_host=\"127.0.0.1\",\n",
        "                local_port=port,\n",
        "                share_token=subdomain,\n",
        "                share_server_address=None\n",
        "            )\n",
        "            return tunnel_url\n",
        "        except Exception as e:\n",
        "            echo(\"启动 Gradio 内网穿透失败\", e)\n",
        "            return None\n",
        "\n",
        "\n",
        "    def start(\n",
        "        self,\n",
        "        ngrok: bool = False,\n",
        "        ngrok_token: Optional[str] = None,\n",
        "        cloudflare: bool = False,\n",
        "        remote_moe: bool = False,\n",
        "        localhost_run: bool = False,\n",
        "        gradio: bool = False,\n",
        "        pinggy_io: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"启动内网穿透\n",
        "\n",
        "        参数:\n",
        "            ngrok (`bool`):\n",
        "                启用 Ngrok 内网穿透\n",
        "\n",
        "            ngrok_token (`str`, `None`):\n",
        "                Ngrok 账号 Token\n",
        "\n",
        "            cloudflare (`bool`):\n",
        "                启用 CloudFlare 内网穿透\n",
        "\n",
        "            remote_moe (`bool`):\n",
        "                启用 remote.moe 内网穿透\n",
        "\n",
        "            localhost_run (`bool`):\n",
        "                使用 localhost.run 内网穿透\n",
        "\n",
        "            gradio (`bool`):\n",
        "                使用 Gradio 内网穿透\n",
        "\n",
        "            pinggy_io (`bool`):\n",
        "                使用 pinggy.io 内网穿透\n",
        "        \"\"\"\n",
        "        if cloudflare or (ngrok and ngrok_token) or remote_moe or localhost_run or gradio or pinggy_io:\n",
        "            echo(\"启动内网穿透\")\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        cloudflare_url = self.cloudflare() if cloudflare else None\n",
        "        ngrok_url = self.ngrok(ngrok_token) if ngrok and ngrok_token else None\n",
        "        remote_moe_url = self.remote_moe() if remote_moe else None\n",
        "        localhost_run_url = self.localhost_run() if localhost_run else None\n",
        "        gradio_url = self.gradio() if gradio else None\n",
        "        pinggy_io_url = self.pinggy_io() if pinggy_io else None\n",
        "\n",
        "        echo(\"下方为 Fooocus 访问地址\")\n",
        "        print(\"==================================================================================\")\n",
        "        print(f\":: CloudFlare: {cloudflare_url}\")\n",
        "        print(f\":: Ngrok: {ngrok_url}\")\n",
        "        print(f\":: remote.moe: {remote_moe_url}\")\n",
        "        print(f\":: localhost_run: {localhost_run_url}\")\n",
        "        print(f\":: Gradio: {gradio_url}\")\n",
        "        print(f\":: pinggy.io: {pinggy_io_url}\")\n",
        "        print(\"==================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "class ENV:\n",
        "    \"\"\"提供初始化环境的功能\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, workspace: Union[str, Path], workfolder: str) -> None:\n",
        "        \"\"\"提供初始化环境的功能\n",
        "\n",
        "        参数:\n",
        "            workspace (`str`, `Path`):\n",
        "                工作区路径\n",
        "\n",
        "            workfolder (`str`):\n",
        "                工作区的文件夹名称\n",
        "        \"\"\"\n",
        "        self.WORKSPACE = workspace\n",
        "        self.WORKFOLDER = workfolder\n",
        "\n",
        "\n",
        "    def prepare_env_depend(self, use_uv: bool = False) -> None:\n",
        "        \"\"\"为 Notebook 的功能准备必须的环境依赖\n",
        "\n",
        "        参数:\n",
        "            use_uv (`bool`):\n",
        "                是否使用 uv 代替 Pip 安装 Python 软件包\n",
        "        \"\"\"\n",
        "        echo(\"安装自身组件依赖\")\n",
        "        !pip install uv\n",
        "        pkg = \"pyngrok pycloudflared gradio-tunneling\"\n",
        "        if use_uv:\n",
        "            !uv pip install {pkg} --system --quiet -U --index-strategy unsafe-best-match || pip install {pkg} -U\n",
        "        else:\n",
        "            !pip install {pkg} -U\n",
        "        !apt update\n",
        "        !apt install aria2 ssh google-perftools -y\n",
        "\n",
        "\n",
        "    def prepare_torch(self, torch_ver: Optional[str] = None, xformers_ver: Optional[str] = None, use_uv: bool = False) -> None:\n",
        "        \"\"\"安装 PyTorch 和 xFormers\n",
        "\n",
        "        参数:\n",
        "            torch_ver (`str`, `None`):\n",
        "                PyTorch 软件包名称和版本信息, 如`torch==2.0.0 torchvision==0.15.1`\n",
        "\n",
        "            xformers_ver (`str`, `None`):\n",
        "                xFormers 软件包名称和版本信息, 如`xformers==0.0.18`\n",
        "\n",
        "            use_uv (`bool`):\n",
        "                使用 uv 代替 Pip 进行 Python 软件包安装\n",
        "        \"\"\"\n",
        "        if use_uv:\n",
        "            if torch_ver:\n",
        "                echo(\"安装 PyTorch\")\n",
        "                !uv pip install {torch_ver} --system --quiet --index-strategy unsafe-best-match || pip install {torch_ver}\n",
        "            else:\n",
        "                echo(\"未指定 PyTorch 包名, 跳过安装\")\n",
        "\n",
        "            if xformers_ver:\n",
        "                echo(\"安装 xFormers\")\n",
        "                !uv pip install {xformers_ver} --no-deps --system --quiet --index-strategy unsafe-best-match || pip install {xformers_ver} --no-deps\n",
        "            else:\n",
        "                echo(\"未指定 xFormers 包名, 跳过安装\")\n",
        "        else:\n",
        "            if torch_ver:\n",
        "                echo(\"安装 PyTorch\")\n",
        "                !pip install {torch_ver}\n",
        "            else:\n",
        "                echo(\"未指定 PyTorch 包名, 跳过安装\")\n",
        "\n",
        "            if xformers_ver:\n",
        "                echo(\"安装 xFormers\")\n",
        "                !pip install {xformers_ver} --no-deps\n",
        "            else:\n",
        "                echo(\"未指定 xFormers 包名, 跳过安装\")\n",
        "\n",
        "\n",
        "    def install_requirements(self, path: Union[str, Path], use_uv: bool = False) -> None:\n",
        "        \"\"\"从文件 (requirements.txt) 中安装 Python 软件包\n",
        "\n",
        "        参数:\n",
        "            path (`str`, `Path`):\n",
        "                依赖记录文件的路径\n",
        "\n",
        "            use_uv (`bool`):\n",
        "                使用 uv 代替 Pip 进行 Python 软件包安装\n",
        "        \"\"\"\n",
        "        import os\n",
        "        if os.path.exists(path):\n",
        "            echo(\"安装依赖\")\n",
        "            if use_uv:\n",
        "                !uv pip install -r \"{path}\" --system --quiet --index-strategy unsafe-best-match || pip install -r \"{path}\"\n",
        "            else:\n",
        "                !pip install -r \"{path}\"\n",
        "        else:\n",
        "            echo(\"依赖文件路径为空\")\n",
        "\n",
        "\n",
        "    def tcmalloc_colab(self) -> None:\n",
        "        \"\"\"配置 TCMalloc 内存优化\"\"\"\n",
        "        echo(\"配置内存优化\")\n",
        "        import os\n",
        "        aria2 = ARIA2(self.WORKSPACE, self.WORKFOLDER)\n",
        "        path = self.WORKSPACE\n",
        "        libtcmalloc_path = os.path.join(self.WORKSPACE, \"libtcmalloc_minimal.so.4\")\n",
        "        aria2.aria2(\"https://github.com/licyk/term-sd/releases/download/archive/libtcmalloc_minimal.so.4\", path, \"libtcmalloc_minimal.so.4\")\n",
        "        os.environ[\"LD_PRELOAD\"] = libtcmalloc_path\n",
        "\n",
        "\n",
        "\n",
        "class MANAGER:\n",
        "    \"\"\"环境管理\"\"\"\n",
        "\n",
        "    def __init__(self, workspace: Union[str, Path], workfolder: str) -> None:\n",
        "        \"\"\"环境管理\n",
        "\n",
        "        参数:\n",
        "            workspace (`str`, `Path`):\n",
        "                工作区路径\n",
        "\n",
        "            workfolder (`str`):\n",
        "                工作区的文件夹名称\n",
        "        \"\"\"\n",
        "        self.WORKSPACE = workspace\n",
        "        self.WORKFOLDER = workfolder\n",
        "\n",
        "\n",
        "    def clear_up(self, clear: Optional[bool] = False) -> None:\n",
        "        \"\"\"清理 Notebook 的内容输出\"\"\"\n",
        "        from IPython.display import clear_output\n",
        "        if clear:\n",
        "            clear_output(wait=False)\n",
        "\n",
        "\n",
        "    def check_gpu(self) -> None:\n",
        "        \"\"\"检查环境中是否有可用的 GPU\"\"\"\n",
        "        echo(\"检测 GPU 是否可用\")\n",
        "        import tensorflow as tf\n",
        "        echo(f\"TensorFlow 版本: {tf.__version__}\")\n",
        "        if tf.test.gpu_device_name():\n",
        "            echo(\"GPU 可用\")\n",
        "        else:\n",
        "            echo(\"GPU 不可用\")\n",
        "            raise Exception(\"\\n没有可用的 GPU, 请在 Colab -> 代码执行程序 > 更改运行时类型 -> 硬件加速器 选择 GPU T4\\n如果不能使用 GPU, 请尝试更换账号!\")\n",
        "\n",
        "\n",
        "    def config_google_drive(self) -> None:\n",
        "        \"\"\"配置 Google Drive\"\"\"\n",
        "        echo(\"检查 Google Drive 是否已挂载\")\n",
        "        import os\n",
        "        if not os.path.exists('/content/drive/MyDrive'):\n",
        "            echo(\"挂载 Google Drive 中, 请根据提示进行操作\")\n",
        "            from google.colab import drive\n",
        "            try:\n",
        "                drive.mount('/content/drive')\n",
        "                echo(\"Google Dirve 挂载完成\")\n",
        "            except Exception as e:\n",
        "                raise Exception(\"挂载 Google Drive 时出现问题\", e)\n",
        "        else:\n",
        "            echo(\"Google Drive 已挂载\")\n",
        "\n",
        "        # 检测并创建输出文件夹\n",
        "        if os.path.exists('/content/drive/MyDrive'):\n",
        "            if not os.path.exists('/content/drive/MyDrive/fooocus_output'):\n",
        "                echo(\"在 Google Drive 创建 fooocus_ouput 文件夹\")\n",
        "                !mkdir -p /content/drive/MyDrive/fooocus_output\n",
        "        else:\n",
        "            raise Exception(\"未挂载 Google Drive, 请重新挂载后重试\")\n",
        "\n",
        "\n",
        "\n",
        "class ModelDownload:\n",
        "    \"\"\"模型下载器\"\"\"\n",
        "\n",
        "    def __init__(self, urls: list) -> None:\n",
        "        \"\"\"模型下载器创建\n",
        "\n",
        "        参数:\n",
        "            urls (`list`):\n",
        "                下载任务\n",
        "\n",
        "        下载队列使用`ARIA2().aria2()`处理, 下载队列的格式如下:\n",
        "        ```python\n",
        "        urls = [\n",
        "            [\"url\", \"path\", \"filename\"],\n",
        "            [\"url\", \"path\", \"filename\"],\n",
        "            [\"url\", \"path\", \"filename\"],\n",
        "        ]\n",
        "        ```\n",
        "        \"\"\"\n",
        "        import threading\n",
        "        import datetime\n",
        "        from queue import Queue\n",
        "        self.urls = urls\n",
        "        self.queue = Queue()\n",
        "        self.aria2 = ARIA2(None, None)\n",
        "        self.datetime = datetime\n",
        "        self.total_urls = len(urls)  # 记录总的 URL 数\n",
        "        self.downloaded_count = 0  # 记录已下载的数量\n",
        "        self.lock = threading.Lock()  # 创建锁以保护对下载计数器的访问\n",
        "\n",
        "\n",
        "    def worker(self) -> None:\n",
        "        \"\"\"调用文件下载工具\"\"\"\n",
        "        while True:\n",
        "            url = self.queue.get()\n",
        "            if url is None:\n",
        "                break\n",
        "            self.aria2.aria2(\n",
        "                url=url[0],\n",
        "                path=url[1],\n",
        "                filename=url[2]\n",
        "            )\n",
        "            self.queue.task_done()\n",
        "            with self.lock:  # 访问共享资源时加锁\n",
        "                self.downloaded_count += 1\n",
        "                self.print_progress()  # 打印进度\n",
        "\n",
        "\n",
        "    def print_progress(self) -> None:\n",
        "        \"\"\"进度条显示\"\"\"\n",
        "        progress = (self.downloaded_count / self.total_urls) * 100\n",
        "        current_time = self.datetime.datetime.now()\n",
        "        time_interval = current_time - self.start_time\n",
        "        hours = time_interval.seconds // 3600\n",
        "        minutes = (time_interval.seconds // 60) % 60\n",
        "        seconds = time_interval.seconds % 60\n",
        "        formatted_time = f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
        "\n",
        "        if self.downloaded_count > 0:\n",
        "            speed = self.downloaded_count / time_interval.total_seconds()\n",
        "        else:\n",
        "            speed = 0\n",
        "\n",
        "        remaining_urls = self.total_urls - self.downloaded_count\n",
        "\n",
        "        if speed > 0:\n",
        "            estimated_remaining_time_seconds = remaining_urls / speed\n",
        "            estimated_remaining_time = self.datetime.timedelta(seconds=estimated_remaining_time_seconds)\n",
        "            estimated_hours = estimated_remaining_time.seconds // 3600\n",
        "            estimated_minutes = (estimated_remaining_time.seconds // 60) % 60\n",
        "            estimated_seconds = estimated_remaining_time.seconds % 60\n",
        "            formatted_estimated_time = f\"{estimated_hours:02}:{estimated_minutes:02}:{estimated_seconds:02}\"\n",
        "        else:\n",
        "            formatted_estimated_time = \"N/A\"\n",
        "\n",
        "        echo(f\"模型下载进度: {progress:.2f}% | {self.downloaded_count}/{self.total_urls} [{formatted_time}<{formatted_estimated_time}, {speed:.2f}it/s]\")\n",
        "\n",
        "\n",
        "    def start_threads(self, num_threads: int = 16) -> None:\n",
        "        \"\"\"启动多线程下载器\n",
        "\n",
        "        参数:\n",
        "            num_threads (`int`):\n",
        "                下载线程, 默认为 16\n",
        "        \"\"\"\n",
        "        import threading\n",
        "        import time\n",
        "        threads = []\n",
        "        self.start_time = self.datetime.datetime.now()\n",
        "        time.sleep(0.1) # 避免 print_progress() 计算时间时出现 division by zero\n",
        "        for _ in range(num_threads):\n",
        "            thread = threading.Thread(target=self.worker)\n",
        "            thread.start()\n",
        "            threads.append(thread)\n",
        "\n",
        "        for url in self.urls:\n",
        "            self.queue.put(url)\n",
        "\n",
        "        self.queue.join()\n",
        "\n",
        "        for _ in range(num_threads):\n",
        "            self.queue.put(None)\n",
        "\n",
        "        for thread in threads:\n",
        "            thread.join()\n",
        "\n",
        "\n",
        "\n",
        "class MIRROR:\n",
        "    \"\"\"PyPI, HuggingFace, Github 镜像管理工具\"\"\"\n",
        "\n",
        "    def __init__(self, workspace: Union[str, Path]) -> None:\n",
        "        \"\"\"设置镜像测试缓存目录和 Git 配置文件目录\"\"\"\n",
        "        self.WORKSPACE = workspace\n",
        "\n",
        "\n",
        "    def set_pypi_index_mirror(self, mirror: Optional[str] = None) -> None:\n",
        "        \"\"\"设置 PyPI Index 镜像源\n",
        "\n",
        "        参数:\n",
        "            mirror (`str`, `None`):\n",
        "                PyPI 镜像源链接, 当不传入镜像源链接时则清除镜像源\n",
        "        \"\"\"\n",
        "        import os\n",
        "        if mirror:\n",
        "            echo(\"使用 PIP_INDEX_URL, UV_INDEX_URL 环境变量设置 PyPI Index 镜像源\")\n",
        "            os.environ[\"PIP_INDEX_URL\"] = mirror\n",
        "            os.environ[\"UV_INDEX_URL\"] = mirror\n",
        "        else:\n",
        "            echo(\"清除 PIP_INDEX_URL, UV_INDEX_URL 环境变量, 取消使用 PyPI Index 镜像源\")\n",
        "            if \"PIP_INDEX_URL\" in os.environ:\n",
        "                del os.environ[\"PIP_INDEX_URL\"]\n",
        "\n",
        "            if \"UV_INDEX_URL\" in os.environ:\n",
        "                del os.environ[\"UV_INDEX_URL\"]\n",
        "\n",
        "\n",
        "    def set_pypi_extra_index_mirror(self, mirror: Optional[str] = None) -> None:\n",
        "        \"\"\"设置 PyPI Extra Index 镜像源\n",
        "\n",
        "        参数:\n",
        "            mirror (`str`, `None`):\n",
        "                PyPI 镜像源链接, 当不传入镜像源链接时则清除镜像源\n",
        "        \"\"\"\n",
        "        import os\n",
        "        if mirror:\n",
        "            echo(\"使用 PIP_EXTRA_INDEX_URL, UV_EXTRA_INDEX_URL 环境变量设置 PyPI Extra Index 镜像源\")\n",
        "            os.environ[\"PIP_EXTRA_INDEX_URL\"] = mirror\n",
        "            os.environ[\"UV_EXTRA_INDEX_URL\"] = mirror\n",
        "        else:\n",
        "            echo(\"清除 PIP_EXTRA_INDEX_URL, UV_EXTRA_INDEX_URL 环境变量, 取消使用 PyPI Extra Index 镜像源\")\n",
        "            if \"PIP_EXTRA_INDEX_URL\" in os.environ:\n",
        "                del os.environ[\"PIP_EXTRA_INDEX_URL\"]\n",
        "\n",
        "            if \"UV_EXTRA_INDEX_URL\" in os.environ:\n",
        "                del os.environ[\"UV_EXTRA_INDEX_URL\"]\n",
        "\n",
        "\n",
        "    def set_pypi_find_links_mirror(self, mirror: Optional[str] = None) -> None:\n",
        "        \"\"\"设置 PyPI Find Links 镜像源\n",
        "\n",
        "        参数:\n",
        "            mirror (`str`, `None`):\n",
        "                PyPI 镜像源链接, 当不传入镜像源链接时则清除镜像源\n",
        "        \"\"\"\n",
        "        import os\n",
        "        if mirror:\n",
        "            echo(\"使用 PIP_FIND_LINKS, UV_FIND_LINKS 环境变量设置 PyPI Find Links 镜像源\")\n",
        "            os.environ[\"PIP_FIND_LINKS\"] = mirror\n",
        "            os.environ[\"UV_FIND_LINKS\"] = mirror\n",
        "        else:\n",
        "            echo(\"清除 PIP_FIND_LINKS, UV_FIND_LINKS 环境变量, 取消使用 PyPI Find Links 镜像源\")\n",
        "            if \"PIP_FIND_LINKS\" in os.environ:\n",
        "                del os.environ[\"PIP_FIND_LINKS\"]\n",
        "\n",
        "            if \"UV_FIND_LINKS\" in os.environ:\n",
        "                del os.environ[\"UV_FIND_LINKS\"]\n",
        "\n",
        "\n",
        "    def set_github_mirror(self, mirror: Optional[Union[str, list]] = None) -> None:\n",
        "        \"\"\"设置 Github 镜像源\n",
        "\n",
        "        参数:\n",
        "            mirror (`str`, `list`, `None`):\n",
        "                Github 镜像源地址, 当传入的是 Github 镜像源地址, 则直接设置 GIT_CONFIG_GLOBAL 环境变量并直接使用该镜像源地址\n",
        "\n",
        "                如果传入的是镜像源列表, 则自动测试可用的 Github 镜像源并设置 GIT_CONFIG_GLOBAL 环境变量\n",
        "\n",
        "                当不传入参数时则清除 GIT_CONFIG_GLOBAL 环境变量并删除 GIT_CONFIG_GLOBAL 环境变量对应的 Git 配置文件\n",
        "\n",
        "        使用:\n",
        "        ```python\n",
        "        set_github_mirror() # 不传入参数时则清除 Github 镜像源\n",
        "\n",
        "        set_github_mirror(\"https://ghfast.top/https://github.com\") # 只设置一个 Github 镜像源时将直接使用该 Github 镜像源\n",
        "\n",
        "        set_github_mirror( # 传入 Github 镜像源列表时将自动测试可用的 Github 镜像源并设置\n",
        "            [\n",
        "                \"https://ghfast.top/https://github.com\",\n",
        "                \"https://mirror.ghproxy.com/https://github.com\",\n",
        "                \"https://ghproxy.net/https://github.com\",\n",
        "                \"https://gh.api.99988866.xyz/https://github.com\",\n",
        "                \"https://gitclone.com/github.com\",\n",
        "                \"https://gh-proxy.com/https://github.com\",\n",
        "                \"https://ghps.cc/https://github.com\",\n",
        "                \"https://gh.idayer.com/https://github.com\",\n",
        "            ]\n",
        "        )\n",
        "        ```\n",
        "        \"\"\"\n",
        "        import os\n",
        "        import subprocess\n",
        "        if not mirror:\n",
        "            echo(\"清除 GIT_CONFIG_GLOBAL 环境变量, 取消使用 Github 镜像源\")\n",
        "            if \"GIT_CONFIG_GLOBAL\" in os.environ:\n",
        "                # 删除对应的配置文件\n",
        "                if os.path.exists(os.environ[\"GIT_CONFIG_GLOBAL\"]):\n",
        "                    path = os.environ[\"GIT_CONFIG_GLOBAL\"]\n",
        "                    !rm -rf \"{path}\"\n",
        "\n",
        "                del os.environ[\"GIT_CONFIG_GLOBAL\"]\n",
        "\n",
        "            return\n",
        "\n",
        "        git_config_path = os.path.join(self.WORKSPACE, \".gitconfig\")\n",
        "        os.environ[\"GIT_CONFIG_GLOBAL\"] = git_config_path\n",
        "\n",
        "        if isinstance(mirror, str):\n",
        "            echo(\"通过 GIT_CONFIG_GLOBAL 环境变量设置 Github 镜像源\")\n",
        "            !git config --global url.\"{mirror}\".insteadOf \"https://github.com\"\n",
        "        elif isinstance(mirror, list):\n",
        "            mirror_test_path = os.path.join(self.WORKSPACE, \"__github_mirror_test__\")\n",
        "            for gh in mirror:\n",
        "                echo(f\"测试 Github 镜像源: {gh}\")\n",
        "                test_repo = f\"{gh}/licyk/empty\"\n",
        "                if os.path.exists(mirror_test_path):\n",
        "                    !rm -rf \"{mirror_test_path}\"\n",
        "                result = subprocess.run(f'git clone {test_repo} \"{mirror_test_path}\"')\n",
        "                if os.path.exists(mirror_test_path):\n",
        "                    !rm -rf \"{mirror_test_path}\"\n",
        "                if result.returncode == 0:\n",
        "                    echo(\"该镜像源可用\")\n",
        "                    !git config --global url.\"{gh}\".insteadOf \"https://github.com\"\n",
        "                    return\n",
        "                else:\n",
        "                    echo(\"镜像源不可用\")\n",
        "\n",
        "            echo(\"无可用的 Github 镜像源, 取消使用 Github 镜像源\")\n",
        "            if os.path.exists(git_config_path):\n",
        "                !rm -rf \"{git_config_path}\"\n",
        "            if \"GIT_CONFIG_GLOBAL\" in os.environ:\n",
        "                del os.environ[\"GIT_CONFIG_GLOBAL\"]\n",
        "        else:\n",
        "            echo(f\"未知镜像源参数类型: {type(mirror)}\")\n",
        "            return\n",
        "\n",
        "\n",
        "    def set_huggingface_mirror(self, mirror: Optional[str] = None) -> None:\n",
        "        \"\"\"设置 HuggingFace 镜像源\n",
        "\n",
        "        参数:\n",
        "            mirror (`str`, `None`):\n",
        "                HuggingFace 镜像源链接, 当不传入镜像源链接时则清除镜像源\n",
        "        \"\"\"\n",
        "        import os\n",
        "        if mirror:\n",
        "            echo(\"使用 HF_ENDPOINT 环境变量设置 HuggingFace 镜像源\")\n",
        "            os.environ[\"HF_ENDPOINT\"] = mirror\n",
        "        else:\n",
        "            echo(\"清除 HF_ENDPOINT 环境变量, 取消使用 HuggingFace 镜像源\")\n",
        "            if \"HF_ENDPOINT\" in os.environ:\n",
        "                del os.environ[\"HF_ENDPOINT\"]\n",
        "\n",
        "\n",
        "    def set_mirror(\n",
        "        self,\n",
        "        pypi_index_mirror: Optional[str] = None,\n",
        "        pypi_extra_index_mirror: Optional[str] = None,\n",
        "        pypi_find_links_mirror: Optional[str] = None,\n",
        "        github_mirror: Optional[Union[str, list]] = None,\n",
        "        huggingface_mirror: Optional[str] = None\n",
        "    ) -> None:\n",
        "        \"\"\"镜像源设置\n",
        "\n",
        "        参数:\n",
        "            pypi_index_mirror (`str`, `None`):\n",
        "                PyPI Index 镜像源链接\n",
        "\n",
        "            pypi_extra_index_mirror (`str`, `None`):\n",
        "                PyPI Extra Index 镜像源链接\n",
        "\n",
        "            pypi_find_links_mirror (`str`, `None`):\n",
        "                PyPI Find Links 镜像源链接\n",
        "\n",
        "            github_mirror (`str`, `list`, `None`):\n",
        "                Github 镜像源链接或者镜像源链接列表\n",
        "\n",
        "            huggingface_mirror (`str`, `None`):\n",
        "                HuggingFace 镜像源链接\n",
        "        \"\"\"\n",
        "        echo(\"配置镜像源中\")\n",
        "        self.set_pypi_index_mirror(pypi_index_mirror)\n",
        "        self.set_pypi_extra_index_mirror(pypi_extra_index_mirror)\n",
        "        self.set_pypi_find_links_mirror(pypi_find_links_mirror)\n",
        "        self.set_github_mirror(github_mirror)\n",
        "        self.set_huggingface_mirror(huggingface_mirror)\n",
        "        echo(\"镜像源配置完成\")\n",
        "\n",
        "\n",
        "\n",
        "class FOOOCUS(ARIA2, GIT, TUNNEL, MANAGER, ENV):\n",
        "    \"\"\"Fooocus 管理工具\"\"\"\n",
        "\n",
        "    def __init__(self, workspace: Union[str, Path], workfolder: str) -> None:\n",
        "        \"\"\"Fooocus 管理工具\n",
        "\n",
        "        参数:\n",
        "            workspace (`str`, `Path`):\n",
        "                工作区路径\n",
        "\n",
        "            workfolder (`str`):\n",
        "                工作区的文件夹名称\n",
        "        \"\"\"\n",
        "        self.WORKSPACE = workspace\n",
        "        self.WORKFOLDER = workfolder\n",
        "        self.tun = TUNNEL(workspace, workfolder, 7865)\n",
        "        self.mirror = MIRROR(self.WORKSPACE)\n",
        "\n",
        "\n",
        "    def get_sd_model(self, url: str, filename: Optional[str] = None) -> None:\n",
        "        \"\"\"下载大模型\n",
        "\n",
        "        参数:\n",
        "            url (`str`):\n",
        "                模型的下载链接\n",
        "\n",
        "            filename (`str`, `None`):\n",
        "                模型下载后保存的名称\n",
        "        \"\"\"\n",
        "        import os\n",
        "        path = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"models\", \"checkpoints\")\n",
        "        filename = url.split(\"/\").pop() if filename is None else filename\n",
        "        super().aria2(url, path, filename)\n",
        "\n",
        "\n",
        "    def get_lora_model(self, url: str, filename: Optional[str] = None) -> None:\n",
        "        \"\"\"下载 LoRA 模型\n",
        "\n",
        "        参数:\n",
        "            url (`str`):\n",
        "                模型的下载链接\n",
        "\n",
        "            filename (`str`, `None`):\n",
        "                模型下载后保存的名称\n",
        "        \"\"\"\n",
        "        import os\n",
        "        path = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"models\", \"loras\")\n",
        "        filename = url.split(\"/\").pop() if filename is None else filename\n",
        "        super().aria2(url, path, filename)\n",
        "\n",
        "\n",
        "    def get_vae_model(self, url: str, filename: Optional[str] = None) -> None:\n",
        "        \"\"\"下载 VAE 模型\n",
        "\n",
        "        参数:\n",
        "            url (`str`):\n",
        "                模型的下载链接\n",
        "\n",
        "            filename (`str`, `None`):\n",
        "                模型下载后保存的名称\n",
        "        \"\"\"\n",
        "        import os\n",
        "        path = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"models\", \"vae\")\n",
        "        filename = url.split(\"/\").pop() if filename is None else filename\n",
        "        super().aria2(url, path, filename)\n",
        "\n",
        "\n",
        "    def get_embedding_model(self, url: str, filename: Optional[str] = None) -> None:\n",
        "        \"\"\"下载 Embedding 模型\n",
        "\n",
        "        参数:\n",
        "            url (`str`):\n",
        "                模型的下载链接\n",
        "\n",
        "            filename (`str`, `None`):\n",
        "                模型下载后保存的名称\n",
        "        \"\"\"\n",
        "        import os\n",
        "        path = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"models\", \"embeddings\")\n",
        "        filename = url.split(\"/\").pop() if filename is None else filename\n",
        "        super().aria2(url, path, filename)\n",
        "\n",
        "\n",
        "    def install_config(self, preset: str, path_config: str, translation: str) -> None:\n",
        "        \"\"\"下载 Fooocus 配置文件\n",
        "\n",
        "        参数:\n",
        "            preset (`str`, `None`):\n",
        "                Fooocus 预设文件下载链接, 下载后将保存在`{self.WORKSPACE}/{self.WORKFOLDER}/presets/custom.json`\n",
        "\n",
        "            path_config (`str`, `None`):\n",
        "                Fooocus 路径配置文件下载链接, 下载后将保存在`{self.WORKSPACE}/{self.WORKFOLDER}/config.txt`\n",
        "\n",
        "            translation (`str`, `None`):\n",
        "                Fooocus 翻译文件下载链接, 下载后将保存在`{self.WORKSPACE}/{self.WORKFOLDER}/language/zh.json`\n",
        "        \"\"\"\n",
        "        import os\n",
        "        path = os.path.join(self.WORKSPACE, self.WORKFOLDER)\n",
        "        preset_path = os.path.join(path, \"presets\")\n",
        "        language_path = os.path.join(path, \"language\")\n",
        "        echo(\"下载配置文件\")\n",
        "        self.aria2(preset, preset_path, \"custom.json\")\n",
        "        self.aria2(path_config, path, \"config.txt\")\n",
        "        self.aria2(translation, language_path, \"zh.json\")\n",
        "\n",
        "\n",
        "    def pre_download_model(self, path: Union[str, Path], thread_num: Optional[int] = 16) -> None:\n",
        "        \"\"\"根据 Fooocus 配置文件预下载模型\n",
        "\n",
        "        参数:\n",
        "            path (`str`, `Path`):\n",
        "                Fooocus 路径\n",
        "        \"\"\"\n",
        "        import os\n",
        "        import json\n",
        "\n",
        "        if os.path.exists(path):\n",
        "            try:\n",
        "                with open(path, \"r\", encoding=\"utf8\") as file:\n",
        "                    data = json.load(file)\n",
        "                    echo(\"预下载 Fooocus 模型中\")\n",
        "            except Exception:\n",
        "                # json 文件格式出现问题\n",
        "                data = {}\n",
        "        else:\n",
        "            data = {}\n",
        "\n",
        "        sd_model_list = data.get(\"checkpoint_downloads\") if not isinstance(data.get(\"checkpoint_downloads\"), type(None)) else {}\n",
        "        lora_list = data.get(\"lora_downloads\") if not isinstance(data.get(\"lora_downloads\"), type(None)) else {}\n",
        "        vae_list = data.get(\"vae_downloads\") if not isinstance(data.get(\"vae_downloads\"), type(None)) else {}\n",
        "        embedding_list = data.get(\"embeddings_downloads\") if not isinstance(data.get(\"embeddings_downloads\"), type(None)) else {}\n",
        "        sd_model_path = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"models\", \"checkpoints\")\n",
        "        lora_path = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"models\", \"loras\")\n",
        "        vae_path = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"models\", \"vae\")\n",
        "        embedding_path = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"models\", \"embeddings\")\n",
        "\n",
        "        model_url = []\n",
        "        model_url += [[sd_model_list.get(i), sd_model_path, i] for i in sd_model_list]\n",
        "        model_url += [[lora_list.get(i), lora_path, i] for i in lora_list]\n",
        "        model_url += [[vae_list.get(i), vae_path, i] for i in vae_list]\n",
        "        model_url += [[embedding_list.get(i), embedding_path, i] for i in embedding_list]\n",
        "\n",
        "        model_downloader = ModelDownload(model_url)\n",
        "        model_downloader.start_threads(num_threads=thread_num)\n",
        "        echo(\"预下载 Fooocus 模型完成\")\n",
        "\n",
        "\n",
        "    def install(\n",
        "        self,\n",
        "        torch_ver: Optional[str] = None,\n",
        "        xformers_ver: Optional[str] = None,\n",
        "        use_uv: bool = False,\n",
        "        pypi_index_mirror: Optional[str] = None,\n",
        "        pypi_extra_index_mirror: Optional[str] = None,\n",
        "        pypi_find_links_mirror: Optional[str] = None,\n",
        "        github_mirror: Optional[Union[str, list]] = None,\n",
        "        huggingface_mirror: Optional[str] = None,\n",
        "        repo: Optional[str] = None,\n",
        "        preset: Optional[str] = None,\n",
        "        path_config: Optional[str] = None,\n",
        "        translation: Optional[str] = None\n",
        "    ) -> None:\n",
        "        \"\"\"安装 Fooocus\n",
        "\n",
        "        参数:\n",
        "            torch_ver (`str`, `None`):\n",
        "                指定的 PyTorch 软件包包名, 并包括版本号\n",
        "\n",
        "            xformers_ver (`str`, `None`):\n",
        "                指定的 xFormers 软件包包名, 并包括版本号\n",
        "\n",
        "            use_uv (`bool`):\n",
        "                使用 uv 替代 Pip 进行 Python 软件包的安装\n",
        "\n",
        "            pypi_index_mirror (`str`, `None`):\n",
        "                PyPI Index 镜像源链接\n",
        "\n",
        "            pypi_extra_index_mirror (`str`, `None`):\n",
        "                PyPI Extra Index 镜像源链接\n",
        "\n",
        "            pypi_find_links_mirror (`str`, `None`):\n",
        "                PyPI Find Links 镜像源链接\n",
        "\n",
        "            github_mirror (`str`, `list`, `None`):\n",
        "                Github 镜像源链接或者镜像源链接列表\n",
        "\n",
        "            huggingface_mirror (`str`, `None`):\n",
        "                HuggingFace 镜像源链接\n",
        "\n",
        "            repo (`str`, `None`):\n",
        "                Fooocus 分支仓库链接\n",
        "\n",
        "            preset (`str`, `None`):\n",
        "                Fooocus 预设文件下载链接, 下载后将保存在`{self.WORKSPACE}/{self.WORKFOLDER}/presets/custom.json`\n",
        "\n",
        "            path_config (`str`, `None`):\n",
        "                Fooocus 路径配置文件下载链接, 下载后将保存在`{self.WORKSPACE}/{self.WORKFOLDER}/config.txt`\n",
        "\n",
        "            translation (`str`, `None`):\n",
        "                Fooocus 翻译文件下载链接, 下载后将保存在`{self.WORKSPACE}/{self.WORKFOLDER}/language/zh.json`\n",
        "        \"\"\"\n",
        "        import os\n",
        "        os.chdir(self.WORKSPACE)\n",
        "        repo = \"https://github.com/lllyasviel/Fooocus\" if repo is None else repo\n",
        "        preset = \"https://github.com/licyk/term-sd/releases/download/archive/fooocus_config.json\" if preset is None else preset\n",
        "        path_config = \"https://github.com/licyk/term-sd/releases/download/archive/fooocus_path_config_colab.json\" if path_config is None else path_config\n",
        "        translation = \"https://github.com/licyk/term-sd/releases/download/archive/fooocus_zh_cn.json\" if translation is None else translation\n",
        "        req_file = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"requirements_versions.txt\")\n",
        "        config_file = os.path.join(self.WORKSPACE, self.WORKFOLDER, \"presets\", \"custom.json\")\n",
        "        self.check_gpu()\n",
        "        echo(f\"Fooocus 内核分支: {repo}\")\n",
        "        echo(f\"Fooocus 预设配置: {preset}\")\n",
        "        echo(f\"Fooocus 路径配置: {path_config}\")\n",
        "        echo(f\"Fooocus 翻译配置: {translation}\")\n",
        "        self.mirror.set_mirror(\n",
        "            pypi_index_mirror=pypi_index_mirror,\n",
        "            pypi_extra_index_mirror=pypi_extra_index_mirror,\n",
        "            pypi_find_links_mirror=pypi_find_links_mirror,\n",
        "            github_mirror=github_mirror,\n",
        "            huggingface_mirror=huggingface_mirror\n",
        "        )\n",
        "        self.prepare_env_depend(use_uv)\n",
        "        self.clone(repo, self.WORKSPACE, self.WORKFOLDER)\n",
        "        self.prepare_torch(torch_ver, xformers_ver, use_uv)\n",
        "        os.chdir(os.path.join(self.WORKSPACE, self.WORKFOLDER))\n",
        "        self.install_requirements(req_file, use_uv)\n",
        "        os.chdir(self.WORKSPACE)\n",
        "        if use_uv: # 纠正 PyTorch 版本\n",
        "            self.prepare_torch(torch_ver, xformers_ver, use_uv)\n",
        "        self.install_config(\n",
        "            preset=preset,\n",
        "            path_config=path_config,\n",
        "            translation=translation\n",
        "        )\n",
        "        self.tcmalloc_colab()\n",
        "        self.pre_download_model(\n",
        "            path=config_file,\n",
        "            thread_num=16\n",
        "        )\n",
        "\n",
        "\n",
        "#######################################################\n",
        "\n",
        "#@markdown ## Fooocus 核心配置选项\n",
        "#@markdown - Fooocus 分支仓库地址：\n",
        "FOOOCUS_REPO = \"https://github.com/lllyasviel/Fooocus\" #@param {type:\"string\"}\n",
        "#@markdown - Fooocus 启动预设文件地址：\n",
        "FOOOCUS_PRESET = \"https://github.com/licyk/term-sd/releases/download/archive/fooocus_config.json\" #@param {type:\"string\"}\n",
        "#@markdown - Fooocus 路径配置地址：\n",
        "FOOOCUS_PATH_CONFIG = \"https://github.com/licyk/term-sd/releases/download/archive/fooocus_path_config_colab.json\" #@param {type:\"string\"}\n",
        "#@markdown - Fooocus 翻译文件下载地址：\n",
        "FOOOCUS_TRANSLATION = \"https://github.com/licyk/term-sd/releases/download/archive/fooocus_zh_cn.json\" #@param {type:\"string\"}\n",
        "#@markdown - Fooocus 启动参数：\n",
        "FOOOCUS_LAUNCH_ARGS = \"--preset custom --language zh --async-cuda-allocation --disable-analytics --always-high-vram\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ## PyTorch 组件版本选项：\n",
        "#@markdown - PyTorch：\n",
        "PYTORCH_VER = \"torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124\" #@param {type:\"string\"}\n",
        "#@markdown - xFormers：\n",
        "XFORMERS_VER = \"xformers==0.0.28.post3\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ## 包管理器选项：\n",
        "#@markdown - 使用 uv 作为 Python 包管理器\n",
        "USE_UV = True #@param {type:\"boolean\"}\n",
        "#@markdown - PyPI 主镜像源\n",
        "PIP_INDEX_MIRROR = \"https://pypi.python.org/simple\" #@param {type:\"string\"}\n",
        "#@markdown - PyPI 扩展镜像源\n",
        "PIP_EXTRA_INDEX_MIRROR = \"https://download.pytorch.org/whl/cu124\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ## 内网穿透选项：\n",
        "#@markdown - 使用 remote.moe 内网穿透\n",
        "USE_REMOTE_MOE = True #@param {type:\"boolean\"}\n",
        "#@markdown - 使用 localhost.run 内网穿透\n",
        "USE_LOCALHOST_RUN = True #@param {type:\"boolean\"}\n",
        "#@markdown - 使用 pinggy.io 内网穿透\n",
        "PINGGY_IO = True #@param {type:\"boolean\"}\n",
        "#@markdown - 使用 CloudFlare 内网穿透\n",
        "USE_CLOUDFLARE = True #@param {type:\"boolean\"}\n",
        "#@markdown - 使用 Gradio 内网穿透\n",
        "USE_GRADIO = True #@param {type:\"boolean\"}\n",
        "#@markdown - 使用 Ngrok 内网穿透（需填写 Ngrok Token，可在 [Ngrok](https://ngrok.com) 官网获取）\n",
        "USE_NGROK = True #@param {type:\"boolean\"}\n",
        "#@markdown - Ngrok Token\n",
        "NGROK_TOKEN = \"2slyasVH47rjEOVaefHl8F2hvKK_2fvF6K9v6fMgQTe2R99wq\"  #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ## 快速启动选项：\n",
        "#@markdown - 配置环境完成后立即启动 Fooocus（并挂载 Google Drive）\n",
        "QUICK_LAUNCH = False #@param {type:\"boolean\"}\n",
        "#@markdown - 不重复配置环境（当重复运行环境配置时将不会再进行安装）\n",
        "NO_REPEAT_CONFIGURE_ENV = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown ## 其他选项：\n",
        "#@markdown - 清理无用日志\n",
        "CLEAR_LOG = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#######################################################\n",
        "\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content\")\n",
        "fooocus = FOOOCUS(\"/content\",\"Fooocus\")\n",
        "\n",
        "try:\n",
        "    _ = fooocus_has_init\n",
        "except:\n",
        "    fooocus_has_init = False\n",
        "\n",
        "\n",
        "if NO_REPEAT_CONFIGURE_ENV:\n",
        "    if not fooocus_has_init:\n",
        "        fooocus.install(\n",
        "            torch_ver=PYTORCH_VER,\n",
        "            xformers_ver=XFORMERS_VER,\n",
        "            use_uv=USE_UV,\n",
        "            pypi_index_mirror=PIP_INDEX_MIRROR,\n",
        "            pypi_extra_index_mirror=PIP_EXTRA_INDEX_MIRROR,\n",
        "            repo=FOOOCUS_REPO,\n",
        "            preset=FOOOCUS_PRESET,\n",
        "            path_config=FOOOCUS_PATH_CONFIG,\n",
        "            translation=FOOOCUS_TRANSLATION,\n",
        "            # pypi_find_links_mirror=PIP_FIND_LINKS_MIRROR, # Colab 的环境暂不需要以下镜像源\n",
        "            # github_mirror=GITHUB_MIRROR,\n",
        "            # huggingface_mirror=HUGGINGFACE_MIRROR\n",
        "        )\n",
        "        INIT_CONFIG = 1\n",
        "        fooocus_has_init = True\n",
        "        fooocus.clear_up(CLEAR_LOG)\n",
        "        echo(\"Fooocus 运行环境配置完成\")\n",
        "    else:\n",
        "        echo(\"检测到不重复配置环境已启用, 并且在当前 Colab 实例中已配置 Fooocus 运行环境, 不再重复配置 Fooocus 运行环境\")\n",
        "        echo(\"如需在当前 Colab 实例中重新配置 Fooocus 运行环境, 请在快速启动选项中取消不重复配置环境功能\")\n",
        "else:\n",
        "    fooocus.install(\n",
        "        torch_ver=PYTORCH_VER,\n",
        "        xformers_ver=XFORMERS_VER,\n",
        "        use_uv=USE_UV,\n",
        "        pypi_index_mirror=PIP_INDEX_MIRROR,\n",
        "        pypi_extra_index_mirror=PIP_EXTRA_INDEX_MIRROR,\n",
        "        repo=FOOOCUS_REPO,\n",
        "        preset=FOOOCUS_PRESET,\n",
        "        path_config=FOOOCUS_PATH_CONFIG,\n",
        "        translation=FOOOCUS_TRANSLATION,\n",
        "        # pypi_find_links_mirror=PIP_FIND_LINKS_MIRROR, # Colab 的环境暂不需要以下镜像源\n",
        "        # github_mirror=GITHUB_MIRROR,\n",
        "        # huggingface_mirror=HUGGINGFACE_MIRROR\n",
        "    )\n",
        "    INIT_CONFIG = 1\n",
        "    fooocus.clear_up(CLEAR_LOG)\n",
        "    echo(\"Fooocus 运行环境配置完成\")\n",
        "\n",
        "if QUICK_LAUNCH:\n",
        "    echo(\"启动 Fooocus 中\")\n",
        "    os.chdir(\"/content/Fooocus\")\n",
        "    fooocus.config_google_drive()\n",
        "    fooocus.tun.start(\n",
        "        ngrok=USE_NGROK,\n",
        "        ngrok_token=NGROK_TOKEN,\n",
        "        cloudflare=USE_CLOUDFLARE,\n",
        "        remote_moe=USE_REMOTE_MOE,\n",
        "        localhost_run=USE_LOCALHOST_RUN,\n",
        "        gradio=USE_GRADIO,\n",
        "        pinggy_io=PINGGY_IO,\n",
        "    )\n",
        "    echo(f\"Fooocus 启动参数: {FOOOCUS_LAUNCH_ARGS}\")\n",
        "    if USE_GRADIO:\n",
        "        !python /content/Fooocus/launch.py {FOOOCUS_LAUNCH_ARGS} --share\n",
        "    else:\n",
        "        !python /content/Fooocus/launch.py {FOOOCUS_LAUNCH_ARGS}\n",
        "    fooocus.clear_up(CLEAR_LOG)\n",
        "    echo(\"已关闭 Fooocus\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZWnei0cZwWzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3756bb51-9747-4eef-cdf7-73e0c2039f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: 模型下载完成\n"
          ]
        }
      ],
      "source": [
        "#@title 👇 下载模型（可选）\n",
        "\n",
        "try:\n",
        "    _ = INIT_CONFIG\n",
        "except:\n",
        "    raise Exception(\"未安装 Fooocus\")\n",
        "\n",
        "# 模型下载\n",
        "echo(\"下载模型中\")\n",
        "#@markdown 选择下载的模型：\n",
        "##############################\n",
        "controlnet-union-sdxl-1.0 = True  #@param {type:\"boolean\"}\n",
        "if sd_xl_base:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/xinsir/controlnet-union-sdxl-1.0/blob/main/diffusion_pytorch_model.safetensors\n",
        "\")\n",
        "\n",
        "sd_xl_base = True  #@param {type:\"boolean\"}\n",
        "if sd_xl_base:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sd_xl_base_1.0_0.9vae.safetensors\")\n",
        "\n",
        "anima_pencil_xl = False  #@param {type:\"boolean\"}\n",
        "if anima_pencil_xl:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animaPencilXL_v200.safetensors\")\n",
        "\n",
        "bluePencilXL  = False  #@param {type:\"boolean\"}\n",
        "if bluePencilXL:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/bluePencilXL_v401.safetensors\")\n",
        "\n",
        "AnythingXL_xl = True  #@param {type:\"boolean\"}\n",
        "if AnythingXL_xl:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/AnythingXL_xl.safetensors\")\n",
        "\n",
        "abyssorangeXLElse_v10 = False  #@param {type:\"boolean\"}\n",
        "if abyssorangeXLElse_v10:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/abyssorangeXLElse_v10.safetensors\")\n",
        "\n",
        "CounterfeitXL  = False  #@param {type:\"boolean\"}\n",
        "if CounterfeitXL:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/CounterfeitXL-V1.0.safetensors\")\n",
        "\n",
        "animeIllustDiffusion  = False  #@param {type:\"boolean\"}\n",
        "if animeIllustDiffusion:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animeIllustDiffusion_v061.safetensors\")\n",
        "\n",
        "nekorayxl = False  #@param {type:\"boolean\"}\n",
        "if nekorayxl:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/nekorayxl_v06W3.safetensors\")\n",
        "\n",
        "animagine_XL_3 = False  #@param {type:\"boolean\"}\n",
        "if animagine_XL_3:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/cagliostrolab/animagine-xl-3.0/resolve/main/animagine-xl-3.0.safetensors\")\n",
        "\n",
        "animagine_XL_3_1 = False  #@param {type:\"boolean\"}\n",
        "if animagine_XL_3_1:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/cagliostrolab/animagine-xl-3.1/resolve/main/animagine-xl-3.1.safetensors\")\n",
        "\n",
        "animagine_XL_4 = False  #@param {type:\"boolean\"}\n",
        "if animagine_XL_4:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/animagine-xl-4.0.safetensors\")\n",
        "\n",
        "heartOfAppleXL_v20 = False  #@param {type:\"boolean\"}\n",
        "if heartOfAppleXL_v20:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/heartOfAppleXL_v20.safetensors\")\n",
        "\n",
        "heartOfAppleXL_v30 = False  #@param {type:\"boolean\"}\n",
        "if heartOfAppleXL_v30:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/heartOfAppleXL_v30.safetensors\")\n",
        "\n",
        "holodayo_xl_21 = False  #@param {type:\"boolean\"}\n",
        "if holodayo_xl_21:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/holodayo-xl-2.1.safetensors\")\n",
        "\n",
        "kivotos_xl_20 = False  #@param {type:\"boolean\"}\n",
        "if kivotos_xl_20:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kivotos-xl-2.0.safetensors\")\n",
        "\n",
        "sd_xl_anime_V52 = False  #@param {type:\"boolean\"}\n",
        "if sd_xl_anime_V52:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/sd_xl_anime_V52.safetensors\")\n",
        "\n",
        "kohakuXLGamma_rev1  = False  #@param {type:\"boolean\"}\n",
        "if kohakuXLGamma_rev1:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/KBlueLeaf/Kohaku-XL-gamma/resolve/main/kohaku-xl-gamma-rev1.safetensors\")\n",
        "\n",
        "kohakuXLBeta_beta7  = False  #@param {type:\"boolean\"}\n",
        "if kohakuXLBeta_beta7:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/kohakuXLBeta_beta7.safetensors\")\n",
        "\n",
        "Kohaku_XL_Delta = False  #@param {type:\"boolean\"}\n",
        "if Kohaku_XL_Delta:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/KBlueLeaf/Kohaku-XL-Delta/resolve/main/kohaku-xl-delta-rev1.safetensors\")\n",
        "\n",
        "kohakuXL_Epsilon = False  #@param {type:\"boolean\"}\n",
        "if kohakuXL_Epsilon:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/KBlueLeaf/Kohaku-XL-Epsilon/resolve/main/kohaku-xl-epsilon-rev1.safetensors\")\n",
        "\n",
        "kohakuXL_Epsilon_rev2 = False  #@param {type:\"boolean\"}\n",
        "if kohakuXL_Epsilon_rev2:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/KBlueLeaf/Kohaku-XL-Epsilon-rev2/resolve/main/kohaku-xl-epsilon-rev2.safetensors\")\n",
        "\n",
        "kohakuXL_Epsilon_rev3 = False  #@param {type:\"boolean\"}\n",
        "if kohakuXL_Epsilon_rev3:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/KBlueLeaf/Kohaku-XL-Epsilon-rev3/resolve/main/kohaku-xl-epsilon-rev3.safetensors\")\n",
        "\n",
        "kohakuXL_Zeta = False  #@param {type:\"boolean\"}\n",
        "if kohakuXL_Zeta:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/KBlueLeaf/Kohaku-XL-Zeta/resolve/main/kohaku-xl-zeta.safetensors\")\n",
        "\n",
        "starryXLV52 = False  #@param {type:\"boolean\"}\n",
        "if starryXLV52:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/starryXLV52_v52.safetensors\")\n",
        "\n",
        "BArtstyleDB = False  #@param {type:\"boolean\"}\n",
        "if BArtstyleDB:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/baxlBartstylexlBlueArchiveFlatCelluloid_xlv1.safetensors\")\n",
        "\n",
        "BArtstyleDB_3 = False  #@param {type:\"boolean\"}\n",
        "if BArtstyleDB_3:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/baxlBlueArchiveFlatCelluloidStyle_xlv3.safetensors\")\n",
        "\n",
        "pony_v6 = False  #@param {type:\"boolean\"}\n",
        "if pony_v6:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/ponyDiffusionV6XL_v6StartWithThisOne.safetensors\")\n",
        "\n",
        "pdForAnime_v20 = False  #@param {type:\"boolean\"}\n",
        "if pdForAnime_v20:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/pdForAnime_v20.safetensors\")\n",
        "\n",
        "tPonynai3_v51 = False  #@param {type:\"boolean\"}\n",
        "if tPonynai3_v51:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/tPonynai3_v51WeightOptimized.safetensors\")\n",
        "\n",
        "omegaPonyXLAnime_v20 = False  #@param {type:\"boolean\"}\n",
        "if omegaPonyXLAnime_v20:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/omegaPonyXLAnime_v20.safetensors\")\n",
        "\n",
        "Illustrious_XL_v01 = False  #@param {type:\"boolean\"}\n",
        "if Illustrious_XL_v01:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v0.1.safetensors\")\n",
        "\n",
        "Illustrious_XL_v10 = False  #@param {type:\"boolean\"}\n",
        "if Illustrious_XL_v10:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/Illustrious-XL-v1.0.safetensors\")\n",
        "\n",
        "noobaiXL_EpsV01 = True  #@param {type:\"boolean\"}\n",
        "if noobaiXL_EpsV01:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_earlyAccessVersion.safetensors\")\n",
        "\n",
        "noobaiXL_EpsV05 = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_EpsV05:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred05Version.safetensors\")\n",
        "\n",
        "noobaiXL_EpsV075 = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_EpsV075:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred075.safetensors\")\n",
        "\n",
        "noobaiXL_EpsV077 = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_EpsV077:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred077.safetensors\")\n",
        "\n",
        "noobaiXL_EpsV10 = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_EpsV10:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred10Version.safetensors\")\n",
        "\n",
        "noobaiXL_EpsV11 = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_EpsV11:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_epsilonPred11Version.safetensors\")\n",
        "\n",
        "noobaiXL_VPre01 = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_VPre01:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPredTestVersion.safetensors\")\n",
        "\n",
        "noobaiXL_VPre05 = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_VPre05:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred05Version.safetensors\")\n",
        "\n",
        "noobaiXL_VPre06 = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_VPre06:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred06Version.safetensors\")\n",
        "\n",
        "noobaiXL_VPre065S = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_VPre065S:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred065SVersion.safetensors\")\n",
        "\n",
        "noobaiXL_VPre075S = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_VPre075S:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred075SVersion.safetensors\")\n",
        "\n",
        "noobaiXL_VPre09R = False  #@param {type:\"boolean\"}\n",
        "if noobaiXL_VPre09R:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred09RVersion.safetensors\")\n",
        "\n",
        "noobaiXL_VPre10 = True  #@param {type:\"boolean\"}\n",
        "if noobaiXL_VPre10:\n",
        "    fooocus.get_sd_model(\"https://huggingface.co/licyk/sd-model/resolve/main/sdxl_1.0/noobaiXLNAIXL_vPred10Version.safetensors\")\n",
        "\n",
        "\n",
        "##############################\n",
        "fooocus.clear_up(CLEAR_LOG)\n",
        "echo(\"模型下载完成\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zbv19T5tbu4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "MbpZVvRMPIAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc86368-4b29-4cea-aefe-4d3bcdffef21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: Nachoneko_2.safetensors 文件已存在，路径: /content/Fooocus/models/loras/Nachoneko_2.safetensors\n"
          ]
        }
      ],
      "source": [
        "#@title 👇 自定义模型下载\n",
        "try:\n",
        "    _ = INIT_CONFIG\n",
        "except:\n",
        "    raise Exception(\"未安装 Fooocus\")\n",
        "\n",
        "#@markdown ### 选择模型种类：\n",
        "model_type = \"LoRA\" # @param [\"Stable Diffusion\", \"LoRA\", \"VAE\", \"Embeding\"]\n",
        "#@markdown ### 填写模型的下载链接：\n",
        "model_url = \"https://huggingface.co/stickland/sd_train/resolve/main/Nachoneko/Nachoneko_2.safetensors\"  #@param {type:\"string\"}\n",
        "#@markdown ### 填写模型的名称（包括后缀名）：\n",
        "model_name = \"Nachoneko_2.safetensors\"  #@param {type:\"string\"}\n",
        "\n",
        "if model_type == \"Stable Diffusion\":\n",
        "    fooocus.get_sd_model(\n",
        "        url=model_url,\n",
        "        filename=model_name if model_name != \"\" else model_url.split(\"/\").pop()\n",
        "    )\n",
        "elif model_type == \"LoRA\":\n",
        "    fooocus.get_lora_model(\n",
        "        url=model_url,\n",
        "        filename=model_name if model_name != \"\" else model_url.split(\"/\").pop()\n",
        "    )\n",
        "elif model_type == \"VAE\":\n",
        "    fooocus.get_vae_model(\n",
        "        url=model_url,\n",
        "        filename=model_name if model_name != \"\" else model_url.split(\"/\").pop()\n",
        "    )\n",
        "elif model_type == \"Embeding\":\n",
        "    fooocus.get_embedding_model(\n",
        "        url=model_url,\n",
        "        filename=model_name if model_name != \"\" else model_url.split(\"/\").pop()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cLB6sKhErcG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263ed415-e7ab-4746-a07a-7bac7b5c723e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: 启动 Fooocus\n",
            ":: 检查 Google Drive 是否已挂载\n",
            ":: Google Drive 已挂载\n",
            ":: 启动内网穿透\n",
            " * Running on https://andrews-apparently-hello-creations.trycloudflare.com\n",
            " * Traffic stats available on http://127.0.0.1:20241/metrics\n",
            ":: 启动 remote.moe 内网穿透失败\n",
            ":: 启动 a.pinggy.io 内网穿透失败\n",
            ":: 下方为 Fooocus 访问地址\n",
            "==================================================================================\n",
            ":: CloudFlare: https://andrews-apparently-hello-creations.trycloudflare.com\n",
            ":: Ngrok: https://39a7-34-105-14-52.ngrok-free.app\n",
            ":: remote.moe: None\n",
            ":: localhost_run: https://73f0882e0222fb.lhr.life\n",
            ":: Gradio: https://d10e1f1dd07e828cbb.gradio.live\n",
            ":: pinggy.io: None\n",
            "==================================================================================\n",
            ":: Fooocus 启动参数: --preset custom --language zh --async-cuda-allocation --disable-analytics --always-high-vram\n",
            "[System ARGV] ['/content/Fooocus/launch.py', '--preset', 'custom', '--language', 'zh', '--async-cuda-allocation', '--disable-analytics', '--always-high-vram', '--share']\n",
            "Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "Fooocus version: 2.5.5\n",
            "Loaded preset: /content/Fooocus/presets/custom.json\n",
            "[Cleanup] Attempting to delete content of temp dir /tmp/fooocus\n",
            "[Cleanup] Cleanup successful\n",
            "Total VRAM 15095 MB, total RAM 12979 MB\n",
            "xformers version: 0.0.28.post3\n",
            "Set vram state to: HIGH_VRAM\n",
            "Always offload VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype: torch.float32\n",
            "Using xformers cross attention\n",
            "Refiner unloaded.\n",
            "[System ARGV] ['/content/Fooocus/launch.py', '--preset', 'custom', '--language', 'zh', '--async-cuda-allocation', '--disable-analytics', '--always-high-vram', '--share']\n",
            "Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "Fooocus version: 2.5.5\n",
            "[Cleanup] Attempting to delete content of temp dir /tmp/fooocus\n",
            "[Cleanup] Cleanup successful\n",
            "Running on local URL:  http://127.0.0.1:7865\n",
            "Running on public URL: https://03cbd200027e76d9cb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "model_type V_PREDICTION\n",
            "UNet ADM Dimension 2816\n",
            "Using xformers attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using xformers attention in VAE\n",
            "extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection'}\n",
            "left over keys: dict_keys(['v_pred', 'ztsnr'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/noobaiXLNAIXL_vPred10Version.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [] for model [/content/Fooocus/models/checkpoints/noobaiXLNAIXL_vPred10Version.safetensors].\n",
            "Fooocus V2 Expansion: Vocab with 642 words.\n",
            "Fooocus Expansion engine loaded for cuda:0, use_fp16 = True.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.06 seconds\n",
            "2025-03-25 13:00:51.914623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742907651.997381   11763 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742907652.021600   11763 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Started worker with PID 11670\n",
            "App started successful. Use the app with http://127.0.0.1:7865/ or 127.0.0.1:7865 or https://03cbd200027e76d9cb.gradio.live\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 8682408224985869577\n",
            "[Parameters] CFG = 5\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = euler_ancestral - sgm_uniform\n",
            "[Parameters] Steps = 30 - 24\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Using xformers attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using xformers attention in VAE\n",
            "extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 2.25 seconds\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/AnythingXL_xl.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [] for model [/content/Fooocus/models/checkpoints/AnythingXL_xl.safetensors].\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.17 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.10 seconds\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1344, 1008)\n",
            "Preparation time: 44.35 seconds\n",
            "Using sgm_uniform scheduler.\n",
            "[Fooocus] Preparing task 1/1 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.18012595176696777, sigma_max = 14.614640235900879\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.70 seconds\n",
            "100% 30/30 [00:39<00:00,  1.30s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.18 seconds\n",
            "[Fooocus] Saving image 1/1 to system ...\n",
            "[Cache] Calculating sha256 for /content/Fooocus/models/checkpoints/AnythingXL_xl.safetensors\n",
            "[Cache] sha256 for /content/Fooocus/models/checkpoints/AnythingXL_xl.safetensors: 8421598e93\n",
            "Image generated with private log at: /content/drive/MyDrive/fooocus_output/2025-03-25/log.html\n",
            "Generating and saving time: 89.75 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 89.75 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 142.62 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.66 seconds\n"
          ]
        }
      ],
      "source": [
        "#@title 👇 启动 Fooocus\n",
        "try:\n",
        "    _ = INIT_CONFIG\n",
        "except:\n",
        "    raise Exception(\"未安装 Fooocus\")\n",
        "\n",
        "echo(\"启动 Fooocus\")\n",
        "import os\n",
        "os.chdir(\"/content/Fooocus\")\n",
        "fooocus.config_google_drive()\n",
        "fooocus.tun.start(\n",
        "    ngrok=USE_NGROK,\n",
        "    ngrok_token=NGROK_TOKEN,\n",
        "    cloudflare=USE_CLOUDFLARE,\n",
        "    remote_moe=USE_REMOTE_MOE,\n",
        "    localhost_run=USE_LOCALHOST_RUN,\n",
        "    gradio=USE_GRADIO,\n",
        "    pinggy_io=PINGGY_IO,\n",
        ")\n",
        "echo(f\"Fooocus 启动参数: {FOOOCUS_LAUNCH_ARGS}\")\n",
        "if USE_GRADIO:\n",
        "    !python /content/Fooocus/launch.py {FOOOCUS_LAUNCH_ARGS} --share\n",
        "else:\n",
        "    !python /content/Fooocus/launch.py {FOOOCUS_LAUNCH_ARGS}\n",
        "fooocus.clear_up(CLEAR_LOG)\n",
        "echo(\"已关闭 Fooocus\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}